<!doctype html>
<html>
<head>
<title>Label2Label: A Language Modeling Framework for Multi-Attribute Learning</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link rel="icon" href="images/logo3.png">
<link href="style.css" rel="stylesheet">
<style>
  .container_2{
    position: relative;
    width: 100%;
    height: 0;
    padding-bottom: 56.25%;
}
.video {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}

  .collapsible {
    background-color: #777;
    color: white;
    cursor: pointer;
    padding: 18px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 15px;
  }

  .active, .collapsible:hover {
    background-color: #555;
  }
  
  .content {
    padding: 0 18px;
    max-height: 0;
    overflow: hidden;
    transition: max-height 0.2s ease-out;
    background-color: #f1f1f1;
  }
</style>

<style>
.paperthumb {
  float:left; width: 120px; margin: 3px 10px 7px 0;
}
.paperdesc {
  clear: both;
}
</style>
</head>

<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <p class="lead" style="font-size:30px">
    <b><a href="https://arxiv.org/abs/2207.08677">Label2Label: A Language Modeling Framework for Multi-Attribute Learning</a></b>
  <address style="font-size: 110%;">
    <nobr><a href="https://li-wanhua.github.io/">Wanhua Li</a>,</nobr>
    <nobr>Zhexuan Cao,</nobr>
    <nobr>Jianjiang Feng,</nobr>
    <nobr>Jie Zhou,</nobr>
    <nobr><a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a></nobr>
  <br>
      <nobr>Tsinghua University</nobr>
  </address>
   <div>European Conference on Computer Vision (ECCV), 2022</div>
 </div>
 </p>
 </div>
</div> <!-- end nd-pageheader -->

<div class="container">

<div class="row">
<div class="col text-center">
<p>
 <a href="https://arxiv.org/pdf/2207.08677.pdf" class="d-inline-block p-3 align-top"><img height="100" width="78" src="images/paper_thumb.png" style="border:1px solid" data-nothumb><br>Paper</a>
 <a href="https://arxiv.org/pdf/2207.08677.pdf" class="d-inline-block p-3 align-top"><img height="100" width="78"  src="images/paper_supp.png" style="border:1px solid" data-nothumb><br>Supplementary</a>
  <a href="https://github.com/Li-Wanhua/Label2Label" class="d-inline-block p-3 align-top"><img height="100" width="78" src="images/github.png" style="border:1px solid" data-nothumb><br>Github Code</a>
 </div>
</div>





<h2>Abstract</h2><hr>
<p>
Objects are usually associated with multiple attributes, and these attributes often exhibit high correlations. 
  Modeling complex relationships between attributes poses a great challenge for multi-attribute learning. 
  This paper proposes a simple yet generic framework named Label2Label to exploit the complex attribute correlations. 
  Label2Label is the first attempt for multi-attribute prediction from the perspective of language modeling. 
  Specifically, it treats each attribute label as a "word" describing the sample. 
  As each sample is annotated with multiple attribute labels, these "words" will naturally form an unordered but meaningful "sentence", 
  which depicts the semantic information of the corresponding sample. Inspired by the remarkable success of pre-training language models in NLP, 
  Label2Label introduces an image-conditioned masked language model, 
  which randomly masks some of the "word" tokens from the label "sentence" and aims to recover them based on the masked "sentence" and the context 
  conveyed by image features. Our intuition is that the instance-wise attribute relations are well grasped if the neural net can infer the missing 
  attributes based on the context and the remaining attribute hints. Label2Label is conceptually simple and empirically powerful. Without incorporating 
  task-specific prior knowledge and highly specialized network designs, our approach achieves state-of-the-art results on three different multi-attribute
  learning tasks, compared to highly customized domain-specific methods. Code is available at https://github.com/Li-Wanhua/Label2Label.
</p>


<h2>Motivation</h2><hr>

<p align="center">
    <img src="images/l2l-mot.png" width="60%">
</p>
<p>For a given sample, many of its attributes are correlated. Modeling complex inter-attribute associations is an important
challenge for multi-attribute learning. To address this challenge, most existing
approaches adopt a multi-task learning framework, which formulates
multi-attribute recognition as a multi-label classification task and simultaneously
learns multiple binary classifiers. We propose a language modeling framework
named Label2Label to model the complex instance-wise attribute relations.
Specifically, we regard an attribute label as a "word", which describes the current
state of the sample from a certain point of view. By treating multiple attribute labels as a “sentence”, we
exploit the correlation between attributes with a language modeling framework.
  </p>


<h2>Overview</h2><hr>
<p align="center">
    <img src="images/l2l-frame.png" width="90%">
</p>
<p>
Given an input image, we first obtain the image
features with a feature extractor. Then the image features are input to the attribute
query network to obtain the predicted label sequence.We randomly mask some "words"
and generate the masked "sentence". We attain a sequence of token embeddings using
a word embedding module. In the end, we recover the entire label "sentence" with a
Transformer decoder module, which is conditioned on the token embeddings and image
features. Although there are some wrong "words" in the pseudo labels, which are shown
in orange, we can treat them as another form of masks.
</p>

  

<h2>Results</h2><hr>

<p>
  Visualization of attention scores among attributes in the self-attention layer.We
show the attention of the first head at layer 1 with two samples. The positive attributes
of each sample are listed in the corresponding bottom-left corner. For the first sample,
that attribute "Male" is strongly related to the presence of "5 o’clock shadow" and the
absence of "Wearing Lipstick" and "Wearing Necklace". For the second sample, the
attribute "Heavy Makeup" assigns more attention to the existence of the attributes
"Wearing Lipstick", "Wearing Necklace", "Attractive", and "Arched Eyebrows".
  </p>
<p align='center'><img src="images/l2l-result.png" width="90%"></p>





<p style="clear:both;">
<div class="card">
<h3 class="card-header">Citation</h3>
<div class="card-block">
<div class="card-text clickselect">
  Wanhua Li, Zhexuan Cao,  Jianjiang Feng, Jie Zhou, and Jiwen Lu. <em>Label2Label: A Language Modeling Framework for Multi-Attribute Learning.</em> European Conference on Computer Vision (ECCV), 2022.
</div>
</div>
</div>
</p>

<p>
<div class="card">
<h3 class="card-header">Bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">
@inproceedings{li2022label2label,
    author    = {Li, Wanhua and Cao, Zhexuan  and Feng, Jianjiang and Zhou, Jie and Lu, Jiwen},
    title     = {Label2Label: A Language Modeling Framework for Multi-Attribute Learning},
    booktitle = {European Conference on Computer Vision (ECCV)},
    year      = {2022},
}
</pre>
</div>
</div>
</p>


<p align="right">
     <a href="https://hanlab.mit.edu/projects/anycost-gan/">Website Template</a>
</p>

</div>
</div> <!-- row -->

</div> <!-- container -->

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = this.nextElementSibling;
      if (content.style.maxHeight){
        content.style.maxHeight = null;
      } else {
        content.style.maxHeight = content.scrollHeight * 50+ "px";
      } 
      content.style.height = "550%";
    });
  }
</script>

</body>
</html>


